{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3496e507",
   "metadata": {},
   "source": [
    "#### Assignment 1 (NLU 22)\n",
    "##### Sentiment analysis using logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55a92f7",
   "metadata": {},
   "source": [
    "In this assignment you train a binary classifier for classifying movie reviews from IMDB database. The task is to classify them as positive or negative. The task has been divided into several subtasks. \n",
    "\n",
    "1. You will need the following libraries. Do not import everything.  \n",
    "    * re\n",
    "    * pandas\n",
    "    * numpy\n",
    "    * scipy\n",
    "    * nltk\n",
    "    * scikit-learn\n",
    "2. The data consists of two directories--positive and negative reviews.   \n",
    "3. Each review is a document. \n",
    "4. Process the texts so that you get rid of punctuation but keeping spaces. We have to be careful with stopwords. Completly removing them may lead to loss of crucial information. (How?) \n",
    "5. You will have to map each document (email) to a vector. \n",
    "6. You will need to use **tf/idf** weighting. You should create the tf-idf vectors from scratch. **Do not use library functions**. \n",
    "7. Once you have the vectors for each document apply logistic regression to the training set to fix the weights. You *may* use **sklearn** logistic regression function from linear models.  \n",
    "8. Test your model with the test set and report accuracy, recall and precision. \n",
    "9. Write a short report (about 250 words) on the model and how one may improve it. \n",
    "10. You will be provided with a set of functions. Your task is to complete them. \n",
    "11. Do not change anything in the structure of these functions. If you have print functions for testing comment them out. \n",
    "12. Commenting your code is important. But not too much commenting. \n",
    "13. The following are the basic steps:\n",
    "    1. process the dataset.\n",
    "    2. build a vocabulary\n",
    "    3. convert documents to vectors by **tf/idf** weighting\n",
    "    4. match the input/output vectors\n",
    "    5. train the model (use logistic regression from sk-learn linear models)\n",
    "    6. test the model and compute performance measures\n",
    "    7. write short report, perhaps start with the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eda9b088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "# these are the libraries you might need. numpy, random will be needed\n",
    "# defaultdict will also prove useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcac91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check out the following. to help you with files\n",
    "r_dir = os.walk(\"imdb_dataset\").__next__()[0]\n",
    "d_dir = os.walk(\"imdb_dataset\").__next__()[1]  \n",
    "data_dirs = [os.path.join(r_dir, d1)  for d1 in d_dir]\n",
    "data_dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273a4283",
   "metadata": {},
   "source": [
    "##### Hints    \n",
    "\n",
    "1. The list data_dirs just contains the directories inside the current location.  \n",
    "2. For example, on Windows os the list should look like *['imdb_dataset\\\\neg2', 'imdb_dataset\\\\pos2']*. \n",
    "3. Store the imdb_dataset wherever you are working. **Do not change the names**. Best to create a separate directory for the assignment and have everything there. \n",
    "4. Do not use absolute paths. If, for example, there is a file \"100.txt\" in the positve directory the path is \"imdb_dataset\\\\pos2\\\\100.txt\". This is the path you should call. \n",
    "5. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00846617",
   "metadata": {},
   "source": [
    "1. The function below splits the two directories (*neg* and *pos*)in ratio 3:1 (75% to 25%) approximately and combines them into 2 lists, say *train* and *test*.\n",
    "2. Both train and test lists should contain positive and negative reviews in approximately equal numbers. Suppose ther are 100 files in \"pos\" and 105 file in \"neg\". Your \"train\" list shold contain arond 150 files about 75 of which are names of postive files. \n",
    "2. You should not create lists containing the texts, only name of the files. You call them when needed. \n",
    "3. Please follow the directory structure given here. \n",
    "    1. The notebook file and the tob directory for the data **imdb_rev** should be in the same directory. \n",
    "    2. **imdb_rev** contains 2 directories *neg* and pos containing negative and positive reviews respectively. \n",
    "    3. So the file-path is \"imdb_rev/neg/filename\" or \"imdb_rev/pos/filename\". \n",
    " 4. Yoy may try random.sample method from library. \n",
    " 5. You should mix up the lists a bit, perhaps using random.shuffle. \n",
    " 6. Since the train/test list contains both types of files you should also keep the \"sentiment\" information. One possibility is to store theem as tuples.Suppose \"00.txt\" is picked up from the \"neg\" directory for the train list. We keep it as (\"00.txt\", 0). This will also help you in building the output (**y**-vector). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225632ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def train_test_split(): \n",
    "#the following lists contain the file names in the 'neg' and 'pos' directories respectively\n",
    "    neg_dir = os.walk(data_dirs[0]).__next__()[2]\n",
    "    pos_dir = os.walk(data_dirs[1]).__next__()[2]\n",
    "    return tr, ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9d2aad",
   "metadata": {},
   "source": [
    "##### Hints\n",
    "1. Explore the function os.walk(*pathname*). \n",
    "2. Try os.walk(*pathname*). __next__(). It gives a list. The first member is the name of the current directory. The **3rd** memebr is the list files in the current directory. Explore. \n",
    "3. Your train-test split should contain the pathnames to the files, not the files themeselves. \n",
    "4. If the instructions are followed the list variable \"neg-dir\" above contains the list of file names in negative directory \"neg2\". \n",
    "5. The train and test lists must contain about equal number of file names from positive and negative directory. \n",
    "6. So the file path is not enough. It must also store the information about the \"sentiment\", whether the file came from negative or positive directory. One simple way to do it is to store a tuple: *(filepath, sentiment)*. You may call the sentiments 0 and 1. For example, if the file \"007.txt\" came from negative directory, the list stores (\"imdb_dataset/neg2/007.txt\", 0) in a linux os or Mac os. You do not have to worry about the os, Python does it automatically. \n",
    "7. Remember to shuffle the list. You may try random.sample(...) (for sampling) and randm.shuffle(...). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deb90bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopWords = sorted(list(stopwords.words('english')))\n",
    "#this gives you all the stopwords including negations like \"no\", \"not\", \"should'nt\"\n",
    "#you must decide how to deal with them, they could be important for the sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278a59ec",
   "metadata": {},
   "source": [
    "*One simple possible way is to modify the list* stopWords and *remove words like \"not\", \"no\", \"wouldn't\", \"couldn;t\" etc.* and *create a new list of stopwords*. Use this modified list to get rid of stopwords. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b48059",
   "metadata": {},
   "source": [
    "1. The following function builds the vocabulary for the training set. \n",
    "2. Vocabulary is the **set** of tokens in all the texts in the \"train\" list. \n",
    "3. Here you have to deal with stopwords. \n",
    "4. It may prove useful to have an auxiliary function, say *proc_text(txt)*. It takes text string as input and produces a *set* of tokens getting rid of the words in modified stopwords list. See below for a possible way of defining such a function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bb237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"stopWords2\" is the new list of stopwords which does not have the \"negation\" words. \n",
    "#this is an auxilliary function\n",
    "def proc_text(txt):\n",
    "    #split the text into tokens, getting rid of all punctuation, use re.split() \n",
    "    #add the empty string '' to the stopWords2 since splitting may produce the empty string \n",
    "    #list of tokens in \"txt\"\n",
    "    tok_list = []\n",
    "    pass\n",
    "#your code goes here. Store the tokens in \"txt\" but not in stopWords2 in tok_list.  \n",
    "    return tok_list\n",
    "#remember the function returns a list so there may be duplications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37551d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(tr):\n",
    "    #vocab = set()\n",
    "    ##you may need to use the function proc_text\n",
    "    ## the vocab_dict updates document frequency, remember a token may ocuur multiple times in a document\n",
    "    ## but it is counted as 1.\n",
    "    # update vocab dict (set as keys, number counted as value) --> look at hints2\n",
    "    vocab_dict = defaultdict(int)\n",
    "    pass\n",
    "#your code \n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3710ff",
   "metadata": {},
   "source": [
    "##### Hints\n",
    "1. Suppose you have made the train and test list of *filepaths*. \n",
    "2. Use the train list to build vocabulary. If you have stored the *filepath* and sentiment as a pair (as sugeested above) then you need the first entry of the pair.  \n",
    "3. Open the files one-by-one and process. **Close** a file before opening the next. The built-in method \"with open(...)\" may be useful. Remember to include encoding. \n",
    "4. Get rid of the stopwords, except the negation types (see above). You may use the auxilliary function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be470a8f",
   "metadata": {},
   "source": [
    "1. Te following function is perhaps the most important for this assignment. Now that we have the vocabulary we create the tf-idf vector for each document. \n",
    "2. This is the map taking a document to a vector. \n",
    "3. The size of the vector will eqaul the length of the vocabulary. \n",
    "4. Put the vector as a *row vector* in the input matrix. \n",
    "5. Suppose the training set has $m$ documents and the size of the vocabulary has $n$ tokens. The input matrix is of the order $m\\times n$. \n",
    "6. Remember each row represents a document. \n",
    "7. You should simultaneausly create the output vector $y$ representing the sentiment of the corresponding document. It is a column vector with dimension $m$. For example, a \"train-set\" with 4 files and vocabulary consisting of 5 tokens will have the form:\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\begin{pmatrix}\n",
    "x_{11} & x_{12} & x_{13} & x_{14} & x_{15} \\\\\n",
    "x_{21} & x_{22} & x_{23} & x_{24} & x_{25} \\\\\n",
    "x_{31} & x_{32} & x_{33} & x_{34} & x_{35} \\\\\n",
    "x_{41} & x_{42} & x_{43} & x_{44} & x_{45} \n",
    "\\end{pmatrix}\n",
    "\\longrightarrow\n",
    "y = \\begin{pmatrix}\n",
    "0\\\\\n",
    "1\\\\\n",
    "0\\\\\n",
    "1\n",
    "\\end{pmatrix} \n",
    "$$\n",
    "8. Here the first row represents the tf-idf vector for document 1 which is negative (0), the second row for document 2 (positive) etc.  \n",
    "\n",
    "9. **Do not use any readymade library functions for tf-idf**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "974c5f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_matrix(train, vocab_dict):\n",
    "    nrow = len(train)\n",
    "    ncol = len(vocab)\n",
    "    toke_dict = defaultdict(int)\n",
    "#update toke_dict\n",
    "    X = np.zeros((nrow, ncol), dtype=np.float32)\n",
    "    y = np.zeros((nrow, 1), dtype=np.float32)\n",
    "    for indx in range(nrow):\n",
    "        tok_list = [] \n",
    "        pass\n",
    "    ## your code, update tok_list, you may use the \"proc_text\" function here\n",
    "        for token in toke_list:\n",
    "            if vocab_dict[toke] == 0:\n",
    "            #this is for the test data, if the token is not there in the vocab ignore it\n",
    "                continue\n",
    "            pass\n",
    "        #compute both the tf-value and idf-value and update X, y\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4119b2ee",
   "metadata": {},
   "source": [
    "##### Hints\n",
    "1. The initial part of the function is similar to building the vocabulary. The document frequency is already there in the output of *build_vocab* function. Call that function seprately and store its output. \n",
    "2. First update \"tok_dict\" using the set *vocab_dict* that is passed on to the function. The index will tell yo where to store the tf-idf values in the matrix X. \n",
    "3. Recall that the entris in the \"train\" list are pairs, the first is the *filepath* and the second is the sentiment. \n",
    "4. It is best to use numbers remembering that the rows represent document index in the train list. For example, if train[0] = (*filepath*, 0) then the tf-idf values of the document in *filepath* will be stored in row 0 of X and y[0] = 0. \n",
    "5. You have to tokenize and process the documents **one-by-one** as in the case of building vocabulary. \n",
    "    1. Open a file.\n",
    "    2. Tokenize it, getting rid of the words in stopWords2. You should have a list of tokens, say \"tok_list, in the document. \n",
    "    3. Count the occurence of each token. You may use built-in \"Counter\" from Python 3 *collections\". \n",
    "    4. Clculate tf-value and store it in X at (*indx*, *tok_dict[token]*). Recall *tok_dict* stores the index of the tokens. \n",
    "    5. Comput the idf-value. \n",
    "    6. Update the appropriate entry of X. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd04df0",
   "metadata": {},
   "source": [
    "1. There are 2 more functions for training and testing using **X** and **y** above.\n",
    "2. These will mostly involve library functions. \n",
    "3. You may use any intermediate \"helper\" functions if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189b2fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training data\n",
    "def apply_logit(X, yt):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    pass\n",
    "#your code goes here, \"es\" is the output of the sklearn LogisticRegression function\n",
    "    return es"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a7b6ab",
   "metadata": {},
   "source": [
    "1. Test the model with the test data X, y. \n",
    "2. You have to create the tf-idf features for the the documents in the test files. Be careful while creating the feature vectors for test files. There may be some tokenns in these files which are not there in the vocabulary. \n",
    "3. You should use the output of the previous function\n",
    "4. hint:look at the \"predict\" function in LogisticRegression\n",
    "5. Now compare the class prediction of the model with the corresponding value given by y. \n",
    "6. Compute the performance metrics *accuracy*, *precision\", *recall* and *F1-score* and report. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd4a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = apply_logit(X, yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fa9050",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_model(X, y):\n",
    "    #use the object \"param\" above, this is the output after training\n",
    "    # you should call the function tf_idf_matrix with the test-list \n",
    "    acc = 0\n",
    "    prec = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    pass\n",
    "#your code\n",
    "    return acc, prec, recall, f1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a72bcc",
   "metadata": {},
   "source": [
    "For your convenience the definition of metrics given below. \n",
    "\n",
    "#### Metrics \n",
    "1. $\\text{precision} = \\frac{tp}{tp + fp} $\n",
    "2. $\\text{recall} = \\frac{tp}{tp + fn} $\n",
    "3. $\\text{accuracy} = \\frac{tp+tn}{tp +tn+ fp+fn} $\n",
    "4. $$ F_\\beta =\\frac{(1 + \\beta^2)\\cdot tp}{(1 + \\beta^2)\\cdot tp + \\beta^2\\cdot fn + fp}$$\n",
    "5. $$F_1 = \\frac{tp}{tp + (fp+fn)/2}$$\n",
    "\n",
    "1. $tp = \\text{number of \"true positive\" } $ \n",
    "2. $tn = \\text{number of \"true negative\" }$\n",
    "3. $fp = \\text{number of \"false positive\" }$ \n",
    "4. $fn = \\text{number of \"false negative\" }$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f31a80f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
