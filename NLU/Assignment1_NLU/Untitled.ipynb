{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e31d526f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imdb_dataset\\\\neg2', 'imdb_dataset\\\\pos2']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "r_dir = os.walk(\"imdb_dataset\").__next__()[0]\n",
    "d_dir = os.walk(\"imdb_dataset\").__next__()[1]  \n",
    "data_dirs = [os.path.join(r_dir, d1)  for d1 in d_dir]\n",
    "data_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbacec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_dir = os.walk(data_dirs[0]).__next__()[2]\n",
    "#print(0.75*len(neg_dir))\n",
    "\n",
    "neg_dirr = random.sample(neg_dir, 2682)\n",
    "\n",
    "pos_dir = os.walk(data_dirs[1]).__next__()[2]\n",
    "pos_dirr = random.sample(pos_dir, 2683)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e884032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec5402b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['this is the first document', 'this document is the second document',\n",
    "         'and this is the third one', 'is this the first document']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24300cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IDF(corpus, unique_words):\n",
    "    idf_dict = {}\n",
    "    N = len(corpus)\n",
    "    for i in unique_words:\n",
    "        count = 0\n",
    "        for sen in corpus:\n",
    "            if i in sen.split():\n",
    "                count = count + 1\n",
    "            idf_dict[i] = (math.log((1+N)/(count+1)))+1\n",
    "    return idf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "899adcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(whole_data):\n",
    "    unique_words = set()\n",
    "    if isinstance(whole_data, (list,)):\n",
    "        for x in whole_data:\n",
    "            for y in x.split():\n",
    "                if len(y)<2:\n",
    "                    continue\n",
    "                unique_words.add(y)\n",
    "    unique_words = sorted(list(unique_words))\n",
    "    vocab = {j:i for i, j in enumerate(unique_words)}\n",
    "    Idf_values_of_all_unique_words=IDF(whole_data, unique_words)\n",
    "    return vocab, Idf_values_of_all_unique_words\n",
    "\n",
    "Vocabulary, idf_of_vocabulary=fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fd5f20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\_index.py:82: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'12' is not a supported norm",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21088/3758785729.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mfinal_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midf_of_vocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21088/3758785729.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(dataset, vocabulary, idf_values)\u001b[0m\n\u001b[0;32m      7\u001b[0m                 \u001b[0mtf_idf_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnumber_of_words_in_sentence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midf_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                 \u001b[0msparse_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_idf_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"NORM FORM\\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparse_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'12'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparse_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'12'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mnormalize\u001b[1;34m(X, norm, axis, copy, return_norm)\u001b[0m\n\u001b[0;32m   1893\u001b[0m     \"\"\"\n\u001b[0;32m   1894\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnorm\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'l1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'l2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'max'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1895\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'%s' is not a supported norm\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1897\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: '12' is not a supported norm"
     ]
    }
   ],
   "source": [
    "def transform(dataset, vocabulary, idf_values):\n",
    "    sparse_matrix = csr_matrix((len(dataset), len(vocabulary)), dtype=np.float64)\n",
    "    for row in range(0, len(dataset)):\n",
    "        number_of_words_in_sentence = Counter(dataset[row].split())\n",
    "        for word in dataset[row].split():\n",
    "            if word in list(vocabulary.keys()):\n",
    "                tf_idf_value = (number_of_words_in_sentence[word]/len(dataset[row].split()))*(idf_values[word])\n",
    "                sparse_matrix[row, vocabulary[word]] = tf_idf_value\n",
    "    print(\"NORM FORM\\n\", normalize(sparse_matrix, norm='12', axis=1, copy=True, return_norm = False))\n",
    "    output = normalize(sparse_matrix, norm='12', axis=1, copy=True, return_norm=False)\n",
    "    return output\n",
    "\n",
    "final_output = transform(corpus, Vocabulary, idf_of_vocabulary)\n",
    "print(final_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6cd3f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2683\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
