{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "186ef73f",
   "metadata": {},
   "source": [
    "## Assignment Natural Language Understanding\n",
    "\n",
    "Elisabeth Putri - 20306250"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d882ff",
   "metadata": {},
   "source": [
    "##### Sentiment analysis using logistic regression\n",
    "\n",
    "Train a binary classifier for classifying movie reviews from IMDB database. The task is to classify them as positive or\n",
    "negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b7527a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "from pathlib import Path\n",
    "from math import floor\n",
    "from collections import Counter\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15633640",
   "metadata": {},
   "source": [
    "After importing the libraries, we are going to access the files directory. First, I create list containing all the text path in imdb directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1f4bf928",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = list(os.walk(\"imdb_dataset\"))\n",
    "data_dirs = []\n",
    "\n",
    "for root, dirs, files in imdb:\n",
    "    data_dirs.append([os.path.join(root, name) for name in files])\n",
    "    \n",
    "# data_dirs[1] is a list of negative sentiments\n",
    "# data_dirs[2] is a list of positive sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5144c6",
   "metadata": {},
   "source": [
    "The datadirs is a combined list containing data from neg2 and pos2. Both of the class can be accessed by list index. Index 0 for data from neg2 and index 1 for data from pos2. The next step is giving an information about their class in each path of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5341b8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "negdata = data_dirs[1]\n",
    "posdata = data_dirs[2]\n",
    "\n",
    "def split(listabc, chunk_size):\n",
    "    for i in range(0, len(listabc), chunk_size):\n",
    "        yield listabc[i:i + chunk_size]\n",
    "\n",
    "splittedneg = list(split(negdata, 1))\n",
    "splittedpos = list(split(posdata, 1))\n",
    "\n",
    "appendedneg = []\n",
    "for n in splittedneg:\n",
    "    appendedneg.append(n + [0])\n",
    "\n",
    "#print(appendedneg)\n",
    "    \n",
    "appendedpos = []\n",
    "for p in splittedpos:\n",
    "    appendedpos.append(p + [1])\n",
    "\n",
    "#print(appendedpos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e26fbc4",
   "metadata": {},
   "source": [
    "After each data has their own information about where their class is, now we are going to combine the data. This compilation is proposed to access each class of data by list index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "20acc4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dirs = [appendedneg, appendedpos]\n",
    "# data_dirs[0]\n",
    "# data_dirs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6788d4cd",
   "metadata": {},
   "source": [
    "The data are now ready to be splitted into train and test dataset as the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d1f67768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "\n",
    "def train_test_split(): \n",
    "    #the following lists contain the file names in the 'neg' and 'pos' directories respectively  \n",
    "    traineg = appendedneg[:floor(0.75*len(appendedneg))]\n",
    "    testneg = appendedneg[floor(0.75*len(appendedneg)):]\n",
    "    \n",
    "    trainpos = appendedpos[:floor(0.75*len(appendedpos))]\n",
    "    testpos = appendedpos[floor(0.75*len(appendedpos)):]\n",
    "    \n",
    "    tr = traineg + trainpos\n",
    "    ts = testneg + testpos\n",
    "    \n",
    "    random.shuffle(tr)\n",
    "    random.shuffle(ts)\n",
    "    return tr, ts\n",
    "\n",
    "tr, ts = train_test_split()\n",
    "tr_size, ts_size = len(tr), len(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd14d1d5",
   "metadata": {},
   "source": [
    "Some negative stopwords are useful for sentiment analysis because it is able to change the meaning of the sentence. Regarding to that, the modified stopwords list are listed as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1097b43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of punctuation but keep the spaces, be careful with the stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = sorted(list(stopwords.words('english')))\n",
    "\n",
    "regex = re.compile(r'[a-z]*n\\'*t*')\n",
    "stopWords2 = [a for a in stopWords if not regex.match(a)]\n",
    "stopWords2.append(' ')\n",
    "#print(stopWords2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99156db8",
   "metadata": {},
   "source": [
    "Creating vocabulary from the training data. The defined function below is porposed to tokenize each text document (proc_text) and also build a vocab dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6f7daab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_text(txt):\n",
    "    tok_list = []\n",
    "    a = txt.translate(str.maketrans('', '', string.punctuation))\n",
    "    txt = re.split(r\"\\s+\", a)\n",
    "    for token in txt:\n",
    "        if not token in stopWords2:\n",
    "            tok_list.append(token.lower()) # to make analysis easier, all the letter are converged into lowercase\n",
    "    return tok_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "e8d735a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(tr):\n",
    "    # building a global vocabulary\n",
    "    words = []\n",
    "    for a in tr:\n",
    "        i = a[0]\n",
    "        with open(i, encoding='utf-8') as f:\n",
    "            b = proc_text(f.read())\n",
    "            words.append(b)\n",
    "    \n",
    "    vodoc = [] # vocabulary in each document\n",
    "    for c in words:\n",
    "        vodoc.append(set(c))\n",
    "        \n",
    "    voc = []   # merging all the subset for each documents into one list\n",
    "    for sublist in vodoc:\n",
    "        for item in sublist:\n",
    "            voc.append(item)\n",
    "    vocab = set(voc)\n",
    "    \n",
    "    # Creating a dictionary\n",
    "    vocab_dict = {}\n",
    "    for e in voc: # we use voc instead of vodoc because we are counting how many document contains each word\n",
    "        vocab_dict[e] = vocab_dict.setdefault(e, 0) + 1\n",
    "\n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c070e80f",
   "metadata": {},
   "source": [
    "After we got the vocab dictionary, we will build the tf-idf function. This function is proposed to produce unique number for each words in vocabulary.The function is used in either train and test dataset. This can be useful function to logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "6275b24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique number for each words in vocabulary\n",
    "\n",
    "def tf_idf_matrix(train, vocab_dict):\n",
    "    nrow = len(train)\n",
    "    ncol = len(vocab_dict)\n",
    "    #update toke_dict \n",
    "    X = np.zeros((nrow, ncol), dtype=np.float32)\n",
    "    y = np.zeros((nrow, 1), dtype=np.float32)\n",
    "    \n",
    "    # tokenizing each given doc text\n",
    "    tok_list = []\n",
    "    for a in train:\n",
    "        i = a[0]\n",
    "        with open(i, encoding='utf-8') as f:\n",
    "            b = proc_text(f.read())\n",
    "            tok_list.append(b)\n",
    "               \n",
    "    # calculating the term frequency\n",
    "    term_frequency_final = []\n",
    "    for idx in range( len(tok_list)):\n",
    "        tok_list_per_idx = tok_list[idx]\n",
    "        list_inner = [tok_list_per_idx]\n",
    "        term_frequency = {}\n",
    "        for o in list_inner:\n",
    "            #print(o)\n",
    "            aa = Counter(o)\n",
    "            #print(aa)\n",
    "            for k, v in aa.items():\n",
    "                term_frequency[k] = v\n",
    "            \n",
    "    term_frequency_final.append(term_frequency)\n",
    "    \n",
    "    # storing term frequency\n",
    "    X = pd.DataFrame(term_frequency_final)\n",
    "    X = df.replace(np.nan,0)\n",
    "\n",
    "    for indx in range(nrow):\n",
    "        tok_list = []\n",
    "        for token in tok_list:\n",
    "            if vocab_dict[token] == 0:\n",
    "                continue\n",
    "    \n",
    "    # computing idf-value\n",
    "    vodoc = [] # vocabulary in each document\n",
    "    for c in tok_list:\n",
    "        vodoc.append(set(c))\n",
    "        \n",
    "    voc = []   # merging all the subset for each documents into one list\n",
    "    for sublist in vodoc:\n",
    "        for item in sublist:\n",
    "            voc.append(item)\n",
    "    \n",
    "    bb_dict = {}\n",
    "    bb = Counter(voc)\n",
    "    for k, v in bb.items():\n",
    "        bb_dict[k] = v\n",
    "\n",
    "    updateidf = ([math.log(len(tr)/value) for value in bb_dict.values()])\n",
    "    keys = list(bb_dict.keys())\n",
    "\n",
    "    idf_dict = dict(zip(keys, updateidf))\n",
    "    \n",
    "    # computing tfidf\n",
    "    tfidf_dict = {}\n",
    "    for a in term_frequency_final:\n",
    "        for b in a.keys():\n",
    "            for c in idf_dict.keys():\n",
    "                if b == c:\n",
    "                    tfidf = a.get(b) * idf_dict.get(b)\n",
    "                    tfidf_dict[b] = tfidf\n",
    "    \n",
    "    # Updating X entry\n",
    "    X = pd.DataFrame(tfidf_dict)\n",
    "    X = df.replace(np.nan, 0)\n",
    "    \n",
    "    # extracting the y vector value\n",
    "    y = []\n",
    "    for a in train:\n",
    "        i = a[1]\n",
    "        y.append(i)\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9ceb2a",
   "metadata": {},
   "source": [
    "After we know the tfidf of each word in the dataset given to that function, we will calculate the Logistic Regression using the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "0e932a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training data to create the model\n",
    "\n",
    "def apply_logit(X, yt):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    global tr\n",
    "    a = build_vocab(tr)\n",
    "    X, y = tf_idf_matrix(tr, a)\n",
    "    ab = LogisticRegression(random_state=0, tol = 0.001, max_iter = 500).fit(X, y)\n",
    "       \n",
    "    ab.predict(yt)\n",
    "    es = ab.get_params\n",
    "    return es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "6c3d2649",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [3, 5364]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7492/2080833291.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0maa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_idf_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mparam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapply_logit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7492/3575874964.py\u001b[0m in \u001b[0;36mapply_logit\u001b[1;34m(X, yt)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_idf_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1342\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1344\u001b[1;33m         X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[0m\u001b[0;32m   1345\u001b[0m                                    \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"C\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m                                    accept_large_sparse=solver != 'liblinear')\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 888\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[0;32m    320\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [3, 5364]"
     ]
    }
   ],
   "source": [
    "test = ts[0:2]\n",
    "aa = build_vocab(ts[0:3])\n",
    "X, yt = tf_idf_matrix(test, aa)\n",
    "param = apply_logit(X, yt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865d7c13",
   "metadata": {},
   "source": [
    "test the model, report accuracy, recall and precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5635c714",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = apply_logit(X, yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "6604f105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(X, y):\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    \n",
    "    if yt == y :\n",
    "        if y == 0:\n",
    "            tp += 1\n",
    "        elif y == 1:\n",
    "            fp += 1\n",
    "    else:\n",
    "        if y == 0:\n",
    "            fn += 1\n",
    "        elif y == 1:\n",
    "            tn += 1\n",
    "                \n",
    "    #use the object \"param\" above, this is the output after training\n",
    "    acc = 0\n",
    "    prec = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    \n",
    "#your code\n",
    "    prec = tp/(tp+fp)\n",
    "    rec = tp/(tp+fn)\n",
    "    acc = (tp+tn)/(tp+tn+fp+fn)\n",
    "    f1 = tp/(tp +(fp+fn)/2)\n",
    "    return acc, prec, recall, f1\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
