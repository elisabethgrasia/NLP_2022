{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lab 8 Exercise  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Download the 6 Austen texts. Find the tf-idf values for the following terms:  \n",
    ">**fight, love, laugh, hate,  joy, happy/happiness, sorrow,  weep, calm, peace, home** \n",
    "\n",
    "2. Make a vector corresponding to the tf-idf value of these terms for each text/document.   **(10)**\n",
    "3. Compute the centroid of all documents and define the document vector    **(5)**   \n",
    "by starting from origin (the vector from the origin to the\n",
    "document). The centroid of $n$ vectors $\\pmb{x}_1, \\pmb{x}_2, \\dotsc, \\pmb{x}_n$ is given by \n",
    "$$ \\frac{\\pmb{x}_1+ \\pmb{x}_2+ \\dotsb, + \\pmb{x}_n}{n} $$  \n",
    "4. Make a matrix of cos() of the angles between these vectors.   **(5)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Answer\n",
    "1. Creating tf-idf for: fight, love, laugh, hate, joy, happy/happiness, sorrow, weep, calm, peace, home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'glob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22028/197414501.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mglob\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"NLU\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"*.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'glob'"
     ]
    }
   ],
   "source": [
    "# Download the 6 Austen texts\n",
    "import os\n",
    "from glob import glob\n",
    "files = glob.glob(os.getcwd(), \"NLU\", \"*.txt\")\n",
    "\n",
    "text = [open(file).read() for file in files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the functions which will be used to find tfidf (given in the lab)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "import string\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "def tokenbuilder(txt):\n",
    "    #stem = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    stem = PorterStemmer( mode = 'NLTK_EXTENSIONS')\n",
    "    txt = txt.lower()\n",
    "    rmv = [\"''\", \"'\", \"'s\", \"``\", \"`\", \"--\", \"__\"]\n",
    "    for token in nltk.word_tokenize(txt):\n",
    "        if token in string.punctuation or token in rmv:\n",
    "            if (token == r'.'):\n",
    "                token = r'</s>'\n",
    "            else:\n",
    "                continue\n",
    "        yield stem.stem(token)\n",
    "\n",
    "def tokenList(txt, stp = 0):\n",
    "    toke_list = [r'<b>']\n",
    "    if (stp == 0):\n",
    "        for token in tokenbuilder(a(txt)):\n",
    "            toke_list.append(token)\n",
    "    else:\n",
    "         for token in tokenbuilder(a(txt)):\n",
    "                if not token in stopWords:\n",
    "                    toke_list.append(token)\n",
    "    return toke_list\n",
    "\n",
    "from collections import defaultdict\n",
    "def vectorize(txt):\n",
    "    features = defaultdict(int)\n",
    "    for token in tokenbuilder(txt):\n",
    "        return features\n",
    "\n",
    "from collections import defaultdict\n",
    "def uniGram(txt, st = 0, rfq = 0):\n",
    "    uniGm = defaultdict(list)\n",
    "    tl = tokenList(txt, st)\n",
    "    #print(tl[len(tl)-2])\n",
    "    for token in tl:\n",
    "        if token in uniGm:\n",
    "            uniGm[token][0] +=1\n",
    "        else: \n",
    "            uniGm[token].append(1)\n",
    "    n = len(uniGm)\n",
    "    print(len(tl), n)\n",
    "    #print(n)\n",
    "    if not (rfq == 0):\n",
    "        for tok in uniGm:\n",
    "            uniGm[tok].append(round(uniGm[tok][0]/n, 3))\n",
    "    return uniGm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22028/2485158924.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0memmatext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muniGram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"emma.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmansfieldtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muniGram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"mansfield\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnorthanger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"northanger.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpersuasion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"persuasion.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpride\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"pride.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22028/1366986358.py\u001b[0m in \u001b[0;36muniGram\u001b[1;34m(txt, st, rfq)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0muniGram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrfq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0muniGm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mtl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[1;31m#print(tl[len(tl)-2])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtl\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22028/1366986358.py\u001b[0m in \u001b[0;36mtokenList\u001b[1;34m(txt, stp)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mtoke_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34mr'<b>'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenbuilder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[0mtoke_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "emmatext = uniGram(\"emma.txt\")\n",
    "mansfieldtext = uniGram(\"mansfield\")\n",
    "northanger = open(\"northanger.txt\", encoding = \"utf-8\")\n",
    "persuasion = open(\"persuasion.txt\", encoding = \"utf-8\")\n",
    "pride = open(\"pride.txt\", encoding = \"utf-8\")\n",
    "sense = open(\"sense.txt\", encoding = \"utf-8\")\n",
    "emmatext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding tfidf\n",
    "# tfidf formula = term frequency of term i in doc j *\n",
    "            # log (total number of documents in the corpus/ number of documents containing term i)\n",
    "    \n",
    "## fight\n",
    "\n",
    "tfidffight = freqfight*(log(6)/ log(dfight))\n",
    "                     \n",
    "## love\n",
    "\n",
    "tfidflove = freqlove*(log(6)/ log(dlove))\n",
    "\n",
    "## laugh\n",
    "\n",
    "tfidflaugh = freqlaugh*(log(6)/ log(dlaugh))\n",
    "\n",
    "## hate\n",
    "\n",
    "tfidfhate = freqhate*(log(6)/ log(dhate))\n",
    "\n",
    "## joy\n",
    "\n",
    "tfidfjoy = freqjoy*(log(6)/ log(djoy))\n",
    "\n",
    "## happy/happiness\n",
    "\n",
    "tfidfhappy = freqhappy*(log(6)/ log(dhappy))\n",
    "\n",
    "# sorrow\n",
    "\n",
    "tfidfsorrow = freqsorrow*(log(6)/ log(dsorrow))\n",
    "\n",
    "# weep\n",
    "\n",
    "tfidfweep = freqweep*(log(6)/ log(dweep))\n",
    "\n",
    "# calm \n",
    "\n",
    "tfidfcalm = freqcalm*(log(6)/ log(dcalm))\n",
    "\n",
    "# peace\n",
    "\n",
    "tfidfpeace = freqpeace*(log(6)/ log(dpeace))\n",
    "\n",
    "# home\n",
    "\n",
    "tfidfhome = freqhome*(log(6)/ log(dhome))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Make a vector corresponding to the tf-idf value of these terms for each text/document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Compute the centroid of all documents and define the document vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Make a matrix of cos() of the angles between these vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
