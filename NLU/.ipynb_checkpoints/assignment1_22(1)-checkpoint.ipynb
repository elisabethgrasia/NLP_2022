{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3496e507",
   "metadata": {},
   "source": [
    "#### Assignment 1 (NLU 22)\n",
    "##### Sentiment analysis using logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55a92f7",
   "metadata": {},
   "source": [
    "In this assignment you train a binary classifier for classifying movie reviews from IMDB database. The task is to classify them as positive or negative. The task has been divided into several subtasks. \n",
    "\n",
    "1. You will need the following libraries. Do not import everything.  \n",
    "    * re\n",
    "    * pandas\n",
    "    * numpy\n",
    "    * scipy\n",
    "    * nltk\n",
    "    * scikit-learn\n",
    "2. The data consists of two directories--positive and negative reviews.   \n",
    "3. Each review is a document. \n",
    "4. Process the texts so that you get rid of punctuation but keeping spaces. We have to be careful with stopwords. Completly removing them may lead to loss of crucial information. (How?) \n",
    "5. You will have to map each document (email) to a vector. \n",
    "6. You will need to use **tf/idf** weighting. You should create the tf-idf vectors from scratch. **Do not use library functions**. \n",
    "7. Once you have the vectors for each document apply logistic regression to the training set to fix the weights. You *may* use **sklearn** logistic regression function from linear models.  \n",
    "8. Test your model with the test set and report accuracy, recall and precision. \n",
    "9. Write a short report (about 250 words) on the model and how one may improve it. \n",
    "10. You will be provided with a set of functions. Your task is to complete them. \n",
    "11. Do not change anything in the structure of these functions. If you have print functions for testing comment them out. \n",
    "12. Commenting your code is important. But not too much commenting. \n",
    "13. The following are the basic steps:\n",
    "    1. process the dataset.\n",
    "    2. build a vocabulary\n",
    "    3. convert documents to vectors by **tf/idf** weighting\n",
    "    4. match the input/output vectors\n",
    "    5. train the model (use logistic regression from sk-learn linear models)\n",
    "    6. test the model and compute performance measures\n",
    "    7. write short report, perhaps start with the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda9b088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "# these are the libraries you might need. numpy, random will be needed\n",
    "# defaultdict will also prove useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406a63f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check out the following. to help you with files\n",
    "r_dir = os.walk(\"imdb_dataset\").__next__()[0]\n",
    "d_dir = os.walk(\"imdb_dataset\").__next__()[1]  \n",
    "data_dirs = [os.path.join(r_dir, d1)  for d1 in d_dir]\n",
    "data_dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1fdec1",
   "metadata": {},
   "source": [
    "1. The function below splits the two directories (*neg* and *pos*)in ratio 3:1 (75% to 25%) approximately and combines them into 2 lists, say *train* and *test*.\n",
    "2. Both train and test lists should contain positive and negative reviews in approximately equal numbers. Suppose ther are 100 files in \"pos\" and 105 file in \"neg\". Your \"train\" list shold contain arond 150 files about 75 of which are names of postive files. \n",
    "2. You should not create lists containing the texts, only name of the files. You call them when needed. \n",
    "3. Please follow the directory structure given here. \n",
    "    1. The notebook file and the tob directory for the data **imdb_rev** should be in the same directory. \n",
    "    2. **imdb_rev** contains 2 directories *neg* and pos containing negative and positive reviews respectively. \n",
    "    3. So the file-path is \"imdb_rev/neg/filename\" or \"imdb_rev/pos/filename\". \n",
    " 4. Yoy may try random.sample method from library. \n",
    " 5. You should mix up the lists a bit, perhaps using random.shuffle. \n",
    " 6. Since the train/test list contains both types of files you should also keep the \"sentiment\" information. One possibility is to store theem as tuples.Suppose \"00.txt\" is picked up from the \"neg\" directory for the train list. We keep it as (\"00.txt\", 0). This will also help you in building the output (**y**-vector). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeac723d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(): \n",
    "#the following lists contain the file names in the 'neg' and 'pos' directories respectively\n",
    "    neg_dir = os.walk(data_dirs[0]).__next__()[2]\n",
    "    pos_dir = os.walk(data_dirs[1]).__next__()[2]\n",
    "    return tr, ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fdf07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopWords = sorted(list(stopwords.words('english')))\n",
    "#this gives you all the stopwords including negations like \"no\", \"not\", \"should'nt\"\n",
    "#you must decide how to deal with them, they could be important for the sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b1b332",
   "metadata": {},
   "source": [
    "1. The following function builds the vocabulary for the training set. \n",
    "2. Vocabulary is the **set** of tokens in all the texts in the \"train\" list. \n",
    "3. Here you have to deal with stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39850194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(tr):\n",
    "    vocab = set()\n",
    "    pass\n",
    "#your code \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5dc2ea",
   "metadata": {},
   "source": [
    "1. The following function is perhaps the most important for this assignment. Now that we have the vocabulary we create the tf-idf vector for each document. \n",
    "2. This is the map taking a document to a vector. \n",
    "3. The size of the vector will eqaul the length of the vocabulary. \n",
    "4. Put the vector as a *row vector* in the input matrix. \n",
    "5. Suppose the training set has $m$ documents and the size of the vocabulary has $n$ tokens. The input matrix is of the order $m\\times n$. \n",
    "6. Remember each row represents a document. \n",
    "7. You should simultaneausly create the output vector $y$ representing the sentiment of the corresponding document. It is a column vector with dimension $m$. For example, a \"train-set\" with 4 files and vocabulary consisting of 5 tokens will have the form:\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\begin{pmatrix}\n",
    "x_{11} & x_{12} & x_{13} & x_{14} & x_{15} \\\\\n",
    "x_{21} & x_{22} & x_{23} & x_{24} & x_{25} \\\\\n",
    "x_{31} & x_{32} & x_{33} & x_{34} & x_{35} \\\\\n",
    "x_{41} & x_{42} & x_{43} & x_{44} & x_{45} \n",
    "\\end{pmatrix}\n",
    "\\longrightarrow\n",
    "y = \\begin{pmatrix}\n",
    "0\\\\\n",
    "1\\\\\n",
    "0\\\\\n",
    "1\n",
    "\\end{pmatrix} \n",
    "$$\n",
    "8. Here the first row represents the tf-idf vector for document 1 which is negative (0), the second row for document 2 (positive) etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0643a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_matrix(train, vocab):\n",
    "    nrow = len(train)\n",
    "    ncol = len(vocab)\n",
    "    X = np.zeros((nrow, ncol), dtype=np.float32)\n",
    "    y = np.zeros((nrow, 1), dtype=np.float32)\n",
    "    pass\n",
    "## your code\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee19bd5b",
   "metadata": {},
   "source": [
    "1. There will be 2 or 3 more functions for training and testing using **X** and **y** above.\n",
    "2. These will mostly involve library functions. \n",
    "3. You may use any intermediate \"helper\" functions if necessary. But they must be called inside one of the above functions only. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
