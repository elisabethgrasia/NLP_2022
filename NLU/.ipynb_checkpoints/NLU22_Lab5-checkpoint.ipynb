{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma = gutenberg.words('austen-emma.txt')\n",
    "emma[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma_tokens = nltk.word_tokenize(gutenberg.raw('austen-emma.txt'))\n",
    "emma_tokens[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caeser = gutenberg.raw('shakespeare-caesar.txt')\n",
    "print(caeser[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import string\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "def tokenbuilder(txt):\n",
    "    #stem = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    stem = PorterStemmer( mode = 'NLTK_EXTENSIONS')\n",
    "    txt = txt.lower()\n",
    "    rmv = [\"''\", \"'\", \"'s\", \"``\", \"`\", \"--\", \"__\"]\n",
    "    for token in nltk.word_tokenize(txt):\n",
    "        if token in string.punctuation or token in rmv:\n",
    "            if (token == r'.'):\n",
    "                token = r'</s>'\n",
    "            else:\n",
    "                continue\n",
    "        #yield stem.stem(token)  \n",
    "        yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def vectorize(txt):\n",
    "    features = defaultdict(int)\n",
    "    for token in tokenbuilder(txt):\n",
    "        features[token] += 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma_f = vectorize(gutenberg.raw('austen-emma.txt'))\n",
    "emma_f['love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma_f['wife']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following function is completed, it creates list of tokens\n",
    "#f(a, b)\n",
    "def tokenList(txt, stp = 0):\n",
    "    toke_list = [r'<b>']\n",
    "    if (stp == 0):\n",
    "        for token in tokenbuilder(txt):\n",
    "            toke_list.append(token)\n",
    "    else:\n",
    "         for token in tokenbuilder(txt):\n",
    "                if not token in stopWords:\n",
    "                    toke_list.append(token)\n",
    "    return toke_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl0 = tokenList(gutenberg.raw('austen-emma.txt'))\n",
    "tl1 = tokenList(gutenberg.raw('austen-emma.txt'), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tl0), len(tl1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Instead of putting everything in one dictionary modify it so that relative frequencies or probabilities are computed only if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the unigram as a dictionary\n",
    "from collections import defaultdict\n",
    "def uniGram(txt, st = 0, rfq = 0):\n",
    "    uniGm = defaultdict(list)\n",
    "    tl = tokenList(txt, st)\n",
    "    #print(tl[len(tl)-2])\n",
    "    for token in tl:\n",
    "        if token in uniGm:\n",
    "            uniGm[token][0] +=1\n",
    "        else: \n",
    "            uniGm[token].append(1)\n",
    "    n = len(uniGm)\n",
    "    print(len(tl), n)\n",
    "    #print(n)\n",
    "    if not (rfq == 0):\n",
    "        for tok in uniGm:\n",
    "            uniGm[tok].append(round(uniGm[tok][0]/n, 3))\n",
    "    return uniGm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = \"\"\"It sent a plume of ash and vapour halfway to space and generated a tsunami that swept across the Pacific.\n",
    "The uncrewed surface vessel will gather data to help researchers understand precisely what happened.\n",
    "Called Maxlimer, the 12m-long robot based in Essex, England, will spend several weeks directly on top of \n",
    "Hunga-Tonga's submerged opening, or caldera, mapping its current shape.\n",
    "It will also lower cameras and instruments to measure environmental conditions, such as the oxygen \n",
    "content of seawater and its turbidity, or cloudiness. These are factors that would impact marine life.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = uniGram(gutenberg.raw('austen-emma.txt'),0, 1)\n",
    "u['loved']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " v = uniGram(st, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the bigram as a dictionary\n",
    "def biGram(txt, st = 0, rfq = 0):\n",
    "    biGm = defaultdict(list)\n",
    "    uGm = uniGram(txt, st)\n",
    "    tl = tokenList(txt, st)\n",
    "    for ti in range(len(tl)-2):\n",
    "        tkpair = (tl[ti], tl[ti+1])\n",
    "        if tkpair in biGm:\n",
    "            biGm[tkpair][0] += 1\n",
    "        else:\n",
    "            biGm[tkpair].append(1)\n",
    "    if not (rfq == 0):\n",
    "        for tp in biGm:\n",
    "            mrg = uGm[tp[0]][0]\n",
    "            biGm[tp].append(round(biGm[tp][0]/mrg, 3))\n",
    "    return biGm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biGrm = biGram(gutenberg.raw('austen-emma.txt'), 0, 1)\n",
    "biGrm[('emma', 'was')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
