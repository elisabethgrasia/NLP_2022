{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### this cell creates the basic dataset, a list, \"names\" you use \n",
    "# as you can see each entry of the list is a list consisting of name and class 'm' or 'f'\n",
    "# you should use only the list \"names\" and no other\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "names_m = [[name.lower().rstrip(), 'm'] for name in nltk.corpus.names.words('male.txt')]\n",
    "names_f = ([[name.lower().rstrip(), 'f'] for name in nltk.corpus.names.words('female.txt')])\n",
    "names_f2 = random.sample(names_f, 3000)\n",
    "names = names_m + names_f2\n",
    "random.shuffle(names)\n",
    "names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## in this function the input list is split into two lists \"train\" and \"test\" approximately \n",
    "## in the ratio 4:1 (about 80% in the train list), the important point is the original list \n",
    "## must be mixed up, so there is a balance between male and female names in both sets \n",
    "global tr_size, ts_size\n",
    "\n",
    "from math import floor\n",
    "def trainTest_split(nam):\n",
    "    train = []\n",
    "    test = []\n",
    "    return (train, test)\n",
    "nb_tr, nb_ts = trainTest_split(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the model 1\n",
    "1. Take as feature the first and the last letter of each name. \n",
    "2. Your program only looks at these two letters. \n",
    "3. Calculate the probabilities for each letter for each class and store it in a dictionary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define two dictionaries below. These will represent the frequencies of different letters in the last and last but 1 position. \n",
    "1. **dict_probF** and **dict_probM** are dictionaries corresponding to female and male names respectively.\n",
    "2. These dictionaries are initilised corresponding to \"add 1\" smoothing. \n",
    "3. The dictionaries actually represent counts. \n",
    "4. When we iterate through the training set we add 1 to the count of character *char* in the last position of dict_probF if the name is in the category 'f' and the last letter of the name is *char*. Similarly for dict_probM for the category 'm'. \n",
    "5. We do the same for letters in the last but one position. \n",
    "6. We have the following correspondence for letters in the last postion (-1):\n",
    "$$ \n",
    "\\begin{split}\n",
    "\\text{dict_probF: }& \\{char, -1: \\frac{count}{(\\text{tr_size} + 26)}\\} \\leftrightarrow P(char, -1|\\text{ 'f'})\\\\\n",
    "\\text{dict_probM: }& \\{char, -1: \\frac{count}{(\\text{tr_size} + 26)}\\} \\leftrightarrow P(char, -1|\\text{ 'm'})\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "8. We have the following correspondence for letters in the last postion (0):\n",
    "$$ \n",
    "\\begin{split}\n",
    "\\text{dict_probF: }& \\{char, 0: \\frac{count}{(\\text{tr_size} + 26)}\\} \\leftrightarrow P(char, -1|\\text{ 'f'})\\\\\n",
    "\\text{dict_probM: }& \\{char, 0: \\frac{count}{(\\text{tr_size} + 26)}\\} \\leftrightarrow P(char, -1|\\text{ 'm'})\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "9. Here tr_size is the size of training set and 26 is the size of \"vocabulary\", the number of letters in English alphabet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialise dictionary of probabilities, \n",
    "2. Since we do not want 0 counts  safe way is to use the add-one trick. That is, initialise the dictionary values with 1.   \n",
    "3. For keys, you may take pairs like (c, 0) or (c,-1). Here 'c' is a character and the numbers denote the position of the letter (0 for first, -1 for last). \n",
    "4. For example, if the name is \"mary\" ('f'), we will have \n",
    "    1. dict_first_F[('m', 0)] += 1\n",
    "    2. dict_first_F[('y', -1)] += 1\n",
    "5. If the name is \"john\" ('m'): \n",
    "    1. dict_first_M[('j', 0)] += 1\n",
    "    2. dict_first_M[('n', -1)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_train(tr):\n",
    "    dict_first_F = {}\n",
    "    dict_first_M = {}\n",
    "    dict_last_F = {}\n",
    "    dict_last_M = {}\n",
    "\n",
    "    \n",
    "    return (dict_first_F, dict_first_M, dict_last_F, dict_last_M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the model 2\n",
    "1. In this case we have 2 features: the first letter and the last letter (maybe same!). Call them $X_1$ and $X_2$. \n",
    "2. $X_1$ stands for the random variable corresponding to the first letter and $X_2$ that for the last letter. \n",
    "3. So given letetrs $c_0$ and $c_1$ and a name $nm$ the task is to calculate \n",
    "$$\\text{Prob}(class(name) = \\text{'f'} \\:\\vert nm[0] = c_1 \\text{ and } nm[-1]= c_2)$$,\n",
    "this is the probability that the given name with first letter $c_1$ and last letter $c_2$ is a female name. \n",
    "4. You can easily infer the probability that $nm$ is male name. \n",
    "5. The decision rule is that whicever class has higher probability. \n",
    "6. We use the naive Bayes rules to calculate these probabilities. \n",
    "7. We estimate the reverse conditionals. \n",
    " \n",
    "$$\\begin{split}\n",
    "&\\text{Prob}(nm[0]  = c_1 \\text{ and } nm[-1]= c_2 \\:\\vert class(name) = \\text{'f'}) \\\\ &= \n",
    "\\text{Prob}(nm[0] = c_1 \\:\\vert class(name) = \\text{'f'})\\text{Prob}(nm[-1]= c_2 \\:\\vert class(name) = \\text{'f'})\n",
    "\\end{split}\n",
    "$$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the following function is completed, it contans the prior probablities\n",
    "## note that we are taking logs\n",
    "\n",
    "def nbPrior(tr):\n",
    "    prF = 0\n",
    "    for en in tr:\n",
    "        if (en[1] == 'f'):\n",
    "            prF =prF+1\n",
    "    log_priorF = np.log(prF/len(nb_tr))\n",
    "    log_priorM = np.log(1 - prF/len(nb_tr))\n",
    "    return (prF, log_priorF, log_priorM)\n",
    "prF, log_priorF, log_priorM = nbPrior(nb_tr)\n",
    "log_priorF, log_priorM, prF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#naive Bayes for the first and last letter\n",
    "nb_probF, nb_probM, nb2_probF, nb2_probM = nb_train(nb_tr)\n",
    "## the following function uses the dictionaries created earlier to calculate probabilities\n",
    "##assume c1 is the last letter and c2 the first. For example, if c1='m' and cnd c2='a', \n",
    "#the function computes the probability that the last character is 'm' and first 'a'. \n",
    "#then it takes the log and adds the prior probabilities\n",
    "def naiveBayes(c1, c2):\n",
    "    lprobF = 0\n",
    "    lprobM = 0\n",
    "    pass \n",
    "## your code\n",
    "    return (log_priorF+lprobF, log_priorM+lprobM)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "##### an example\n",
    "Your values may differ, don't worry about it!  \n",
    "lF, lM = naiveBayes('e', 's')  \n",
    "lF, lM = -4.98, -5.35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics \n",
    "1. $\\text{precision} = \\frac{tp}{tp + fp} $\n",
    "2. $\\text{recall} = \\frac{tp}{tp + fn} $\n",
    "3. $\\text{accuracy} = \\frac{tp+tn}{tp +tn+ fp+fn} $\n",
    "4. $$ F_\\beta =\\frac{(1 + \\beta^2)\\cdot tp}{(1 + \\beta^2)\\cdot tp + \\beta^2\\cdot fn + fp}$$\n",
    "5. $$F_1 = \\frac{tp}{tp + (fp+fn)/2}$$\n",
    "6. $tp$ = \"true positive\", $tn$ = \"true negative\", $fp$ = \"false positive\", $fn$ = \"false negative\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the metrics. Take the test set, use the naiveBayes function to predict whether \n",
    "# a name belongs to 'f' or 'm' class. Compare with the actual value. \n",
    "# Suppose we instead of 'f' and 'm'we class them as 'positive' and 'negative'. \n",
    "#If the name is 'f' (positive) and your function predicts 'f' then \"true positive\".\n",
    "#If the name is 'm' (positive) and your function predicts 'f' then \"false positive\".\n",
    "#If the name is 'm' (positive) and your function predicts 'm' then \"true negative\".\n",
    "def metrics(ts):\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    pass \n",
    "### your code, calculate the above numbers\n",
    "    prec = tp/(tp+fp)\n",
    "    rec = tp/(tp+fn)\n",
    "    acc = (tp+tn)/(tp+tn+fp+fn)\n",
    "    F1_f = tp/(tp +(fp+fn)/2)\n",
    "    F1_m = tn/(tn +(fp+fn)/2)\n",
    "    return(prec, rec, acc, F1_f, F1_m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
