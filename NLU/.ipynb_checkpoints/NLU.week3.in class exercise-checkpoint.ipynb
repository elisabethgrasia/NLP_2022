{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cbd2b8a",
   "metadata": {},
   "source": [
    "#### Lab 3 in-class exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "990a5881",
   "metadata": {},
   "outputs": [],
   "source": [
    "### A simple word counting exercise\n",
    "s = \"\"\"UN scientists are likely to weigh up technology to remove CO2 from the atmosphere, \n",
    "as they gather to finalise a key report. This idea will be one of many solutions considered \n",
    "over the next two weeks by the Intergovernmental Panel on Climate Change (IPCC).\n",
    "Also in attendance will be government officials from all over the world, \n",
    "who will need to approve every line in the summary report.\n",
    "\n",
    "This new study will be the third of three important reports from the IPCC issued over the \n",
    "past eight months. The previous two have looked at the causes and impacts of climate change, \n",
    "but this one will focus on mitigation - or what we can do to stop it.\n",
    "\n",
    "This essentially means that researchers will look at how we can reduce the amount of \n",
    "warming gases that are emitted from human activities. The kind of carbon removal approaches  \n",
    "the report will consider will likely include tree planting and agriculture, as well as the  \n",
    "more advanced technological approaches that use large machines to remove the carbon  \n",
    "from the air. \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81742b87",
   "metadata": {},
   "source": [
    "1. Write function which takes a string as input, tokenizes it and outputs a dictionary giving the count of each distinct token. \n",
    "2. Remove the stopwords from the tokens and do the counting again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c3caece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import nltk\n",
    "\n",
    "def vectorize(txt):\n",
    "    features = defaultdict(int)\n",
    "    for token in tokenbuilder(txt):\n",
    "        features[token] += 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9210a66f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "len(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f790d6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "\n",
    "def tokenList(txt, stp = 0):\n",
    "    toke_list = [r'<b>']\n",
    "    if (stp == 0):\n",
    "        for token in tokenbuilder(txt):\n",
    "            toke_list.append(token)\n",
    "    else:\n",
    "        for token in tokenbuilder(txt):\n",
    "            if not token in stopWords:\n",
    "                toke_list.append(token)\n",
    "    return toke_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca4bab0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'defaultdict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22220/299423338.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'emma'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22220/80844007.py\u001b[0m in \u001b[0;36mvectorize\u001b[1;34m(txt)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenbuilder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'defaultdict' is not defined"
     ]
    }
   ],
   "source": [
    "feat = vectorize('emma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a235b8d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
