{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lab 6 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Conditioning in probability models\n",
    "\n",
    "1. Conditioning means we have some additional information and we must recalculate probabilties incorporating this. \n",
    "2. The additional information may mean that we have knowldge that an event has occurred. Then the conditioning is on the event. Consider some examples. \n",
    "    * A dice is thrown and you are told that an even number came up (additional information). \n",
    "    * Suppose we have 2 binary strings of same length, say, 20. The first string has 9, 0's and the second 14. \n",
    "      A coin is tossed and if heads come up string 1 is generated, otherwise string 2. If you know the outcome \n",
    "      of the toss then probability of picking up 0 or 1 when a random character from the string is chosen is easy to compute. \n",
    "3. The additional information maybe information about the value of a *random variable*. This is  subtle point and we will try to understand it through some simple examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Calculating condtional probabilities\n",
    "1. Recall probabilities assign numbers to events. Events are subsets of the sample or probability space, the set of all outcomes.  \n",
    "2. If $\\Omega$ is the sample space (set of all outcomes) and $A \\subset B$, then $P(A)$ will denote the probability of event $A$. How we assign these numbers $P(A)$ is part of the probability model. \n",
    "    * In a coin-tossing game, the event space $\\Omega = \\{0, 1\\}$ (or $\\{T, H\\}$), 0 for tails and 1 for heads. If we assume in a coin toss that the coin is fair then $P({0}) = P({1}) = 0.5$. \n",
    "    * For a dice game $\\Omega = \\{1, 2, 3, 4, 5, 6 \\}$. If it is a fair dice the probability of any single outcome is 1/6. Probability of an even number coming up is \n",
    "    $$ P(\\{2, 4, 6 \\}) = \\frac{1}{2} $$\n",
    "    Here the event, \"an even outcome\", is the set $\\{2, 4, 6 \\}$. \n",
    "3. If $A, B$ are events then the joint event $A$ *and* $B$ is the intersection $AB$ (or $A\\cap B$). \n",
    "4. The condtional probability that $B$ occurs given $A$ is \n",
    "$$ P(B|A) = \\frac{P(AB)}{P(A)}, \\; P(A) \\neq = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Compute the probability of 6 coming up conditioned event that an even number came up. Use the definition. See example above. \n",
    "2. Let $X$ be an **rv** (random variable) associated with coin toss in the second example. It takes value 0 for \"tails\" and 1 for \"heads\". Let $B$ be the vent that character picked at random from the string is 0. \n",
    "    * What is the probability of $B$ if we don't know the value of $X$, in other words we don't know the outcome of the toss. \n",
    "    * What is the conditional probability given that $X=1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Bayes theorem\n",
    "1. It is often the case that the conditional probability $P(B|A)$ can be computed easily, but we are interested in the probability $P(A|B)$. \n",
    "2. From the definition $P(A|B) = \\frac{P(AB)}{P(B)}$ and $P(B|A) = \\frac{P(AB)}{P(A)}$. Thus\n",
    "$$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $$\n",
    "This is **Bayes theorem**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Some terminology\n",
    "In Bayes theorem $P(A)$ is called the prior probability of $A$ and $P(B)$ is the uncondtional probability of $B$. $P(A|B)$ is the posterior probability of $A$. It is so-called because we now have probabilty of $A$ posterior to the event that $B$ has occured. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Naive Bayes classifier 1\n",
    "1. Suppose we extract features $f_1, f_2, \\dotsc, f_k$ from each document. This means that each document is characterized by its features. \n",
    "2. Also suppose that the each document can be classified into one of the $m$ classes. Let us denote them $1, 2, \\dotsc, m$. \n",
    "3. Our problem is to find the class to which an unclassified document $d$ belongs. \n",
    "$$ P(c|d) = P(c|f_1, f_2, \\dotsc, f_k) $$\n",
    "where $f_1, f_2, \\dotsc, f_k$ are the features of $d$. \n",
    "4. From Bayes theorem \n",
    "$$  P(c|f_1, f_2, \\dotsc, f_k) \\propto  P(f_1, f_2, \\dotsc, f_k|c)P(c) \\text{ (proportional to)}$$\n",
    "The proportionality constant is not relevant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Naive Bayes classifier 2\n",
    "1. So we have to compute $P(c)$ and $ P(f_1, f_2, \\dotsc, f_k|c)$. \n",
    "2. Computing $P(c)$ is simple. We use the training set and the fraction of those in class $c$ is taken as $P(c)$. For example, if $m=3$ (three classes) and about 25% are in class 1. Then $P(C=1) = .25$. \n",
    "3. To compute $ P(f_1, f_2, \\dotsc, f_k|c)$ we make the extra assumption that for each document the features are conditionally independent (the naive part!). This implies that \n",
    "$$  P(f_1, f_2, \\dotsc, f_k|c) = P(f_1|c)P(f_2|c) \\dotsb P(f_k|C)$$ \n",
    "4. So the problem reduces to calculating $P(f_i |c)$, given a class $c$ what is the probability that a feature is present.\n",
    "5. \n",
    "\n",
    "$$ P(f_i|c) = \\frac{\\mathrm{count}(f_i, c) +1 } { \\sum_i \\mathrm{count}(f_i, c) + k} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes classifier 3\n",
    "1. Sometimes we do not use counts but whether a feature is present or absent. \n",
    "2. Then instead of count we have 1 if the feature occurs and 0 otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### The problem \n",
    "1. Decide what you want to consider as features. \n",
    "2. Divide the datasets into two subsets for training and testing. \n",
    "3. Compute $P(c)$ and $P(f_i|c)$ from the training set. \n",
    "4. Test the model on the test set. To determine the class of a document, we use **maximum likelihood** estimate. \n",
    "$$ \\hat{c} = \\underset{c}{\\mathrm{argmax}} P(c|d) = \\underset{c}{\\mathrm{argmax}}(\\log{(P(c))} + \\sum_i \\log{P(f_i|c))} $$\n",
    "5. Used the naive Bayes assumption and taken logs. \n",
    "6. Calculate *accuracy*, *precision* etc. for the model. \n",
    "7. To improve these meausres go back to features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
